---
title: "Occupancy Prediction"
author: "Jessica Steinemann234"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

This data can be downloaded from the University of California, Irvine's [machine learning repository](http://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+).

### Imports

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(pROC)
library(lubridate)
library(broom)
```

### Cleaning data

```{r warning=FALSE, message=FALSE}
read_and_clean <- function(f){
  df <- read_csv(f, skip = 1) # Skip the first row
  df <- df[,-1] # Remove the first column
  col_names <- read_csv(f, n_max=0) %>% names() # Retrieve the column names as a separate csv file
  names(df) <- col_names
  return(df)
}

retrieve_data <- function(data_loc){
  # Get a list of all the files in data_loc
  files <- list.files(data_loc, full.names = TRUE)
  
  # Iterate through all files, applying the read_and_clean function
  list_df <- map(files, read_and_clean) 
  
  # Bind together the data frames
  df <- bind_rows(list_df)
  
  return(df)
}

df <- retrieve_data('data/occupancy')

# Show first few rows
head(df)
```

### Plotting

```{r}
df %>% 
  filter(month(as_date(date)) == 2) %>% 
  ggplot(aes(x = date, y = Occupancy, fill = Occupancy)) + 
  geom_line(size = 1, color = 'darkgreen') + 
  theme_bw()
```

### Select Columns

```{r}
df_clean <- df %>%
  select(Occupancy, Humidity, CO2, Temperature) %>%
  mutate(Occupancy = as.factor(Occupancy))
```

### Split Train/Test

```{r}
set.seed(456)

train_idx <- sample(1:nrow(df_clean), 0.8*nrow(df_clean))
train <- df_clean[train_idx,]
test <- df_clean[-train_idx,]
```

### EDA

```{r}
train %>%
  pivot_longer(Humidity:Temperature,
               names_to = 'variable') %>% 
  ggplot(aes(x = Occupancy, y = value)) + 
  geom_boxplot() + 
  facet_wrap(~variable, scales = 'free_y')
```

### Fitting a Logistic Regression Model (glm)

```{r}
log_reg <- glm(Occupancy ~ ., data = train, family = 'binomial')
```

### Evaluating on Test Data

```{r}
test_pred <- test %>% 
  mutate(pred = predict(log_reg, newdata = test, type = 'response'),
         pred_class = as.integer(pred > 0.5),
         correct = pred_class == Occupancy) 
```

### Error Metrics

```{r}
classification_rate <- mean(test_pred$correct)
print(classification_rate)
```

### Receiver Operator Curve

```{r}
plot(roc(test_pred$Occupancy, test_pred$pred), print.auc = TRUE)
```

### Confusion Matrix

```{r}
conf_matrix <- table(test_pred$pred_class, test_pred$Occupancy) # True class (columns); predicted class (rows)

conf_matrix
```

### Rescaling the confusion matrix as proportions

```{r}
conf_matrix / colSums(conf_matrix)
```

Notice that sensitivity (true positive rate) is a lot lower than the true negative rate. This is due to the dataset being imbalanced.

### Showing the imbalance

There are over 3x as many observations of where `Occupancy == 0` as compared with `Occupancy == 1`.

```{r}
count(train, Occupancy)
```

### Plotting Regression Coefs

The following code plots the coefficients of the fitted model. There are error bars (created using the standard error) but in this case they may be hidden underneath the points!

```{r}
tidy(log_reg) %>% 
  ggplot(aes(x = term, 
             y = estimate, 
             ymin = estimate - std.error,
             ymax = estimate + std.error)) + 
  geom_pointrange(size = 1)
```

### Downsampling

The following function can be used to randomly downsample a data frame by Occupancy. It uses completely random sampling. There are more sophisticated methods which aim to do this in a more robust (but still random) way to preserve variance in the dataset.

The output of this chunk shows that after the downsampling function is applied, the classes are balanced.

```{r}
downsample_by_occupancy <- function(df){
  ## Function for downsampling a data frame to even out class membership 
  ## (specifically by occupancy)
  
  # Determine the majority class
  class_counts <- count(df, Occupancy)
  majority_class <- class_counts$Occupancy[which.max(class_counts$n)]
  
  # Calculate difference in observations between majority/minority classes (how many rows to remove)
  n_to_downsample <- abs(diff(class_counts$n))
  
  # Add rownames
  df <- df %>% rownames_to_column()
  
  # Sample rows from majority class to remove
  to_remove <- df %>% 
    filter(Occupancy == majority_class) %>%
    sample_n(n_to_downsample)
  
  # Use anti_join to remove those rows
  downsampled_df <- df %>% anti_join(to_remove)
  
  # Remove rownames
  downsampled_df <- downsampled_df %>% select(-rowname)
  
  return(downsampled_df)
}

# Testing the downsampling function
train %>% 
  downsample_by_occupancy() %>% 
  count(Occupancy)
```

### A More Robust and Intepretable Model

In the following code we will apply two pre-processing techniques to achieve build a more robust model (thanks to downsampling) that is more intepretable too (due to standardisation):

-   Downsampling
-   Standardisation

```{r}
set.seed(2021)

# Downsample the data
train_downsampled <- train %>% 
  downsample_by_occupancy() 

# Function for standardising a data frame relative to training data 
standardise_df <- function(df, training_df){
  # For loop over columns
  for (col in names(df)){
    # Check if column is numeric (not applicable to character, factor etc.)
    if (is.numeric(df[[col]])){
      mu <- mean(training_df[[col]]) # mean of vector in training data
      sigma <- sd(training_df[[col]]) # sd of vector in training data
      df[[col]] <- (df[[col]] - mu) / sigma # standardise
    }
  }
  return(df)
}

# Apply the standardisation function
train_stand <- standardise_df(train_downsampled, train_downsampled)

# Fit the model to the processed data
log_reg <- glm(Occupancy ~ ., data = train_stand, family = 'binomial')

# Standardise the test data, with reference to the (downsampled) training data
test_stand <- standardise_df(test, train_downsampled)

# Add predictions
test_stand <- test_stand %>% 
  mutate(pred = predict(log_reg, newdata = test_stand, type = 'response'),
         pred_class = as.integer(pred > 0.5),
         correct = pred_class == Occupancy)

# Create confusion matrix
conf_matrix <- table(test_stand$pred_class, test_stand$Occupancy) 
conf_matrix / colSums(conf_matrix)

classification_rate <- mean(test_stand$correct)
print(classification_rate)

# Plot the ROC
plot(roc(test_stand$Occupancy, test_pred$pred), print.auc = TRUE)

# Plot the coefficients
tidy(log_reg) %>% 
  ggplot(aes(x = term, 
             y = estimate, 
             ymin = estimate - std.error,
             ymax = estimate + std.error)) + 
  geom_pointrange()

```
